## Transformer Implementation details


## TODO:

1. Implement the modified Embedding Layer
2. Implement the Positional Encoding Layer 
3. Implement the Encoder
    - Attention Mechanism
    - MultiHeadAttention Layer
    - Layer Normalization
    - Input Masking
4. Implement the Decoder
    - Encoder-Decoder Attention
    - Decoder Masking
   
5. Implement custom arabic tokenizer
6. There is a memory leak in training loop $\implies$ Need to be investigated