# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

## Keywords:
- Language model
- Bidirectional representation
- Fine-tuning

## Section 0: Abstract
**BERT** is a language model, based on the Transformer architecture. It can be fine-tuned to create sota NLP models.
The thought behind **BERT** is to train a large language model in an unsupervised manner. Then, use the pretrained model as a backbone for a more simpler Task specific architecture (downstream task).

- 7.7 % improvement on GLUE benchmark.
- 4.6 % MultiNLI improvement.
- Question answering
    - SQuAD v1 1.5 improvement (_f1_ 93.2).
    - SQuAD v2 5.1 improvement (_f1_ 83.1).

