# Learning Natural Language Processing


## Papers:

- ["BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"](https://arxiv.org/abs/1810.04805) (EMNLP 2018)
- ["ELMO: Deep contextualized word representations"](https://arxiv.org/abs/1802.05365) (NAACL 2018)
- ["Attention Is All You Need"](https://arxiv.org/abs/1706.03762) (NIPS 2017)
- ["Generative Pre-training"](https://arxiv.org/abs/1711.09534) (ICLR 2018)
- ["BERT-based models for language understanding"](https://arxiv.org/abs/1908.08962) (ACL 2019)
- ["RoBERTa: A Robustly Optimized BERT Pretraining Approach"](https://arxiv.org/abs/1907.11692) (arXiv 2019)
- ["ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"](https://arxiv.org/abs/1909.11942) (ICLR 2020)
- ["MaskGAN: Better Text Generation via Filling in the______"](https://arxiv.org/abs/1801.07736)
